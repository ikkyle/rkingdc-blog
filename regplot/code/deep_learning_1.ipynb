{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "Our last classsifier was very poor--it operated at chance--a coin flip would have had the same predictive power. A few things may have been going on that casued us to find no signal. I could be we down sampled our images too much and lost useful information, it could be that our model was poorly configured (it was), it could be we were using thr wrong model (we were), or it could be all of these. My gut (and eyes) say there is still plenty of information in our down sampled images, so we're going to try a more approproate model before we go back and try with larger images. \n",
    "\n",
    "We're going to build a convolutional neural network using keras (with Tensorflow as the backend). For starters, convolutional neural networks are much better at image classification than the mutli-layer perceptron used in the last post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# keeps warnings from printing for nicer blog output\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Permute, Reshape, LSTM, Dropout, TimeDistributed, Dense, Activation, Flatten\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.expanduser('~/share/rkingdc-blog/regplot'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this is the same file we saw in the previous post, but with the image data as an added column. Each cell in that `img_series` column is a numpy ndarray of shape `(168,168)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "We'll start by building our classifier. Code will be explained inline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 256, 768)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256, 150)          551400    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256, 150)          180600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256, 150)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 256, 100)          15100     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 25601     \n",
      "=================================================================\n",
      "Total params: 772,701\n",
      "Trainable params: 772,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim1 = 256\n",
    "lstm_size = 150\n",
    "hidden_layer_size = 100\n",
    "adam_parms = {'lr': 1e-4, 'beta_1': 0.9, 'beta_2': 0.999}\n",
    "\n",
    "mod = Sequential()\n",
    "\n",
    "mod.add(Permute((2,1,3), input_shape=(input_dim1,input_dim1,3)))\n",
    "mod.add(Reshape(target_shape = (input_dim1,input_dim1*3)))\n",
    "\n",
    "# our hidden layers\n",
    "mod.add(LSTM(lstm_size, return_sequences=True))\n",
    "mod.add(LSTM(lstm_size, return_sequences=True))\n",
    "\n",
    "# dropout \n",
    "mod.add(Dropout(0.5))\n",
    "\n",
    "mod.add(TimeDistributed(Dense(hidden_layer_size), input_shape=(input_dim1, lstm_size) ))\n",
    "\n",
    "mod.add(Flatten())\n",
    "\n",
    "mod.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "mod.compile(optimizer=optimizers.Adam(**adam_parms), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesing Data\n",
    "\n",
    "In image processing we'll want to pre-proccess our images before we train a model on them, by adding some random stretching, blurring, rotating, etc. Keras has utilities included to make this easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale = 1/255)\n",
    "test_gen = ImageDataGenerator(rescale = 1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 images belonging to 2 classes.\n",
      "Found 4000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_gen.flow_from_directory('data/imgs/train',\n",
    "                                      shuffle=True,\n",
    "                                      batch_size=36,\n",
    "                                      class_mode='binary')\n",
    "test = test_gen.flow_from_directory('data/imgs/test',\n",
    "                                    shuffle=True,\n",
    "                                    batch_size=36,\n",
    "                                    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f031ccea9b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.fit_generator(train,\n",
    "       epochs=10,\n",
    "       verbose=0,\n",
    "       validation_data=test,\n",
    "       callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "mod.save(f'trained_model_1_{str(date.today())}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "[0.02431063937046565, 0.9917499966025353]\n"
     ]
    }
   ],
   "source": [
    "model_eval = mod.evaluate_generator(test, use_multiprocessing=True, workers=2)\n",
    "print(mod.metrics_names)\n",
    "print(model_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regplot",
   "language": "python",
   "name": "regplot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
